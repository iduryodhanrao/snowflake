{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ruygvqmwzxmtyr3umd3l",
   "authorId": "4796547911560",
   "authorName": "ADMIN",
   "authorEmail": "mats.stellwall@snowflake.com",
   "sessionId": "2306418d-143d-465e-b30c-b3bac8ab93bf",
   "lastEditTime": 1746449929811
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "b4de75e0-d371-4c67-b1a3-c88a542ec88f",
   "metadata": {
    "language": "python",
    "name": "py_streamlit_import"
   },
   "outputs": [],
   "source": "import streamlit as st",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b3e3fe0-c477-42c5-a3e3-554712bd9647",
   "metadata": {
    "language": "python",
    "name": "py_headline",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.markdown(\"\"\"\n<div style=\"background-color: #000020; color: white; text-align: center; padding: 20px\">\n  <h1 style=\"margin: 0; color: white\"><b>Questioning the Answers: LLMs enter the Boardroom</b></h1>\n  <h2 style=\"margin: 0; color: white\"><b></b>Using Gen AI Tools to Harness Alpha from Earnings Calls</h2>\n  <h3 style=\"margin: 0; color: white\"><b>by S&P Global Market Intelligence's Quantitative Research & Solutions (QRS) Group</b></h3>\n</div>\n\"\"\", unsafe_allow_html=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04b2912a-e563-47a2-8731-7bba38772e76",
   "metadata": {
    "name": "md_1_overview",
    "collapsed": false
   },
   "source": "# 1. Overview\nEarnings calls play a pivotal role in shaping investor perceptions. The quality of communication between executives and analysts can significantly influence company performance. On-topic and proactive exeutives, who deliver proactive presentations, anticipate market queries, and provide clear, on-topic answers to analysts’ questions—consistently outperform their peers. Conversely, off-topic and reactive executives, who fail to address analysts’ key inquiries during presentations, and provide off-topic responses—significantly underperform.\n\nExecutives' ability to anticipate investor concerns and maintain a focused dialogue fosters confidence and strategic communication. In contrast, failing to provide clarity when analysts seek additional information can lead to misalignment and breakdowns in transparency. A long (short) portfolio of on-topic and proactive (off-topic and reactive) generates +515bps of annualized alpha.\n\nThis notebook serves as a introduction for the research detailed in Quantitative Research & Solutions’ recent publication, [\"Questioninig the Answers: LLM's enter the Boardroom.\"](https://www.spglobal.com/market-intelligence/en/news-insights/research/questioning-the-answers-llms-enter-the-boardroom) IIt analyse executive on-topicness and proactiveness using the analysts questions, executives answers and LLM answers. This research harness alpha using LLM tools, including vector embeddings, vector cosine similarity, and the LLM quesiton answering. There is a longer version avalible upon request that also covers how to create the input data from the datasets described in section 2, please reach out to QRS@spglobal.com for access to the longer version."
  },
  {
   "cell_type": "markdown",
   "id": "f5a4c140-634d-4a3d-9db3-bf2febbbbafc",
   "metadata": {
    "name": "md_2_datasets",
    "collapsed": false
   },
   "source": "# 2. Datasets\n\nThe [\"Questioninig the Answers: LLM's enter the Boardroom.\"](https://www.spglobal.com/market-intelligence/en/news-insights/research/questioning-the-answers-llms-enter-the-boardroom) research is using the datasets below from the Snowflake Marketplace. Access to those are not neccessary for running this QuickStart, where we are using a sample datase.\n\nTo reproduce the full research using the complete datasets then request access to those below using the links or contact SnowflakeMarketplace@spglobal.com.\n\n|Name|Description |\n|----|----|\n|[ S&P Capital IQ Financials](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D2N/s-p-global-market-intelligence-s-p-capital-iq-financials)|S&P Capital IQ Financials provides global standardized financial statement data for over 180,000 companies, including over 95,000 active and inactive public companies, and As Reported data for over 150,000 companies. S&P Capital IQ Standardized Financials allows you to extend the scope of your historical analysis and back-testing models with consistent data from all filings of a company's historical financial periods including press releases, original filings, and all restatements.|\n|[Global Events](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D38/s-p-global-market-intelligence-global-events)|The Global Events dataset provides details on upcoming and past corporate events such as earnings calls, shareholder/analyst meetings, expected earnings release dates and more. With deep history back to 2003, clients can leverage this dataset to derive signals and support trading models across asset classes, trading styles and frequencies. This dataset also helps in research & analysis, risk management & compliance, and trade surveillance workflows.|\n|[Machine Readable Transcripts](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D2V/s-p-global-market-intelligence-machine-readable-transcripts)|The Machine Readable Transcripts dataset aggregates data from earnings calls delivered in a machine-readable format for Natural Language Processing (NLP) applications with metadata tagging. Leverage Machine Readable Transcripts to keep track of event information for specific companies including dates, times, dial-in and replay numbers and investor relations contact information. Easily combine data from earnings, M&A, guidance, shareholder, company conference presentations and special calls with traditional datasets to develop proprietary analytics.|\n|[Compustat® Financials](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D2R/s-p-global-market-intelligence-compustat®-financials)|Compustat Financials provides standardized North American and global financial statements and market data for over 80,000 active and inactive publicly traded companies that financial professionals have relied on for over 50 years. Compustat allows investment professionals, academic researchers, and industry analysts to combine deep history with robust and consistent data standardization into their research and backtesting to produce valuable insights and generate alpha. With historical data for North America as far back as 1950 and point-in-time snapshots beginning in 1987, Compustat provides you with insight into company financial performance across many different economic cycles not available anywhere else.|"
  },
  {
   "cell_type": "markdown",
   "id": "fa040835-3299-4600-b06f-9a47ff3312f7",
   "metadata": {
    "name": "md_3_libraries",
    "collapsed": false
   },
   "source": "# 3. Libraries & User Inputs\nImport libraries required for the workflow\n\n## 3.1 Libraries\n\nBefore running, mak sure you have added **cachetools** through **packages**"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "py_imports",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "# Import python packages\nimport json\n\nfrom snowflake.snowpark import functions as snow_funcs\nfrom snowflake.snowpark import Window\n\nfrom snowflake.cortex import embed_text_768, complete, CompleteOptions, summarize\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Mapping of functions that is not exposed in the Snowpark API\nsnf_ifnull = snow_funcs.function(\"IFNULL\")\nsnf_vector_cosine_similarity = snow_funcs.function(\"vector_cosine_similarity\")\nsnf_count_tokens = snow_funcs.function(\"SNOWFLAKE.CORTEX.COUNT_TOKENs\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c4187b8f-34a3-402c-8c63-96623f676453",
   "metadata": {
    "name": "md_32_user_inputs",
    "collapsed": false
   },
   "source": "## 3.2 User Inputs\n\nThis research invloves the usage of an embedding model and a completion model, the default models were set to \"snowflake-arctic-embed-m\" for embedding and \"llama3.1-8b\" for completion. This user input section gives you the flexibility to chose your own model for the task."
  },
  {
   "cell_type": "code",
   "id": "fe9edb8d-b06e-4394-acd8-ba517c9b98d6",
   "metadata": {
    "language": "python",
    "name": "py_user_inputs",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Which embedding model we want to use, see https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#availability \n# for avalible embedding models in Snowflake\nembedding_model = \"snowflake-arctic-embed-m\" \n\n# Which LLM we want to use, see https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#availability \n# for avalible LLMs in Snowflake\ncompletion_model = \"llama3.1-8b\" \n\n# Name of the databse created in the Setup Snowflake step\nsp_llm_qs_location = \"SP_LLM_QS.PUBLIC\"\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "46d68789-dc5f-4bb6-ba5f-dbe16efe1546",
   "metadata": {
    "name": "md_4_Sentence_Tokenization",
    "collapsed": false
   },
   "source": "# 4. S&P Global Q4 2024 Earnings Call Transcript\n\nThe dataset we are going to use are the prepared remarks, questions and answers sections of the S&P Global Q4 2024 Earnings Call. They have been tokenized on sentence level, each row has one sentence in the PROCESSEDTEXT column and the COMPONENTTEXT has the full text for the prepared remarks, questions and answers."
  },
  {
   "cell_type": "code",
   "id": "dbb70cf7-7f25-4840-a00d-843622b0e802",
   "metadata": {
    "language": "python",
    "name": "py_check_source"
   },
   "outputs": [],
   "source": "all_component_df = session.table(f\"{sp_llm_qs_location}.SAMPLE_TRANSCRIPT\")\nall_component_df.sort(\"COMPONENTORDER\", \"SENTENCEORDER\").limit(50)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ed66a3e-f47b-4e7a-92b6-ffd992018e02",
   "metadata": {
    "name": "md_5_RAG",
    "collapsed": false
   },
   "source": "# 5. Working with the Data: Retrival Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is a tool that improves LLM consistency by retrieving relevant information before answering a question.\n\nFor this task the LLM needs to be as consistent as possible in its responses to the analysts’ questions as inconsistency will lead to variations in cosine similarity scores and disrupt feature generation downstream.\n\nTo combat this, we designed a Retrieval-Augmented Generation (RAG) engine that chunks the prepared remarks sentence-by-sentence and retrieves the optimal retrieval percentage of sentences most similar to the question. Inconsistency occurs when the LLM is provided with too little (or too much) context, it becomes uninformed (unspecific). The optimal retrieval percentage for consistency is 60%.\n\nIn the cells below, we vector embed all questions, prepared remark sentences and answer sentences using the snowflake-arctic-m embedding model. Then use the cosine similarity from the (question vs prepared remark sentences) and (question vs answer sentences) to select top 60% most relevant prepared remark and answer sentences to the question from S&P Global Q4 2024 Earnings Call Transcript\n\n## 5.1 S&P Global Q4 2024 Earnings Call Transcript\nSelect and seperate the prepared remarks, questions and answers sections of the S&P Global Q4 2024 Earnings Call"
  },
  {
   "cell_type": "code",
   "id": "164a0ce9-7d9e-40f2-b412-d0058ca3abcd",
   "metadata": {
    "language": "python",
    "name": "py_get_remarks_questions_answers",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get all Prepared Remarks,\nprepared_remarks = all_component_df.filter(snow_funcs.col('transcriptComponentTypeId') == 2) \n# Get all Analyst Questions\nquestions = all_component_df.filter(snow_funcs.col('transcriptComponentTypeId') == 3)\n# Get all Answers\nanswers = all_component_df.filter(snow_funcs.col('transcriptComponentTypeId') == 4) \n\nst.dataframe(prepared_remarks.limit(5))\nst.dataframe(questions.limit(5))\nst.dataframe(answers.limit(5))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7a7838f1-a7b4-430b-bc2f-e1fd32af6e44",
   "metadata": {
    "name": "md_52_Vector_Embedding",
    "collapsed": false
   },
   "source": "## 5.2 Vector Embedding\nTransform the questions, prepared remark sentences and answer sentences into numerical\nrepresentations using the snowflake-arctic-m embedding model.\n\n### 5.2.2 Apply Embedding to Transcript Components"
  },
  {
   "cell_type": "code",
   "id": "2b5eb4ac-7f94-48da-91b9-a407f3bdb5ca",
   "metadata": {
    "language": "python",
    "name": "py_create_components_embeddings",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Generate embeddings for the prepared remarks using the embed_text_768 function and store them in a new column, PrepRemarkSentenceVec\nprepared_remarks_vec_df = (prepared_remarks\n                            .withColumn(\"PrepRemarkSentenceVec\"\n                                        , embed_text_768(snow_funcs.lit(embedding_model),prepared_remarks[\"PROCESSEDTEXT\"]))\n                           ).cache_result()\n# Generate embeddings for the questions using the embed_text_768 function and store them in a new column, questionVec\nquestions_vec_df = (questions\n                        .withColumn(\"questionSentenceVec\"\n                                    , embed_text_768(snow_funcs.lit(embedding_model),questions[\"PROCESSEDTEXT\"]))\n                  ).cache_result()\n# Generate embeddings for the answers using the embed_text_768 function and store them in a new column, sentenceVec\nanswers_vec_df = (answers\n                    .withColumn(\"answerSentenceVec\"\n                                , embed_text_768(snow_funcs.lit(embedding_model),answers[\"PROCESSEDTEXT\"]))\n                        ).cache_result()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aca42ae7-a9b7-4713-8c91-ee5148ef347c",
   "metadata": {
    "name": "md_53_Cosine_Similarity",
    "collapsed": false
   },
   "source": "# 5.3 Cosine Similarity\nTo determine semantic closeness, we vector-embed the question-and-answer texts and calculate a cosine similarity score between the two vectors.\nFor example, A and B each represent a vector such that:\n\n\n$$\tA = [a_1,a_2,… a_n] $$\n$$\tB = [b_1,b_2,… b_n] $$\n\n\nThe cosine similarity formula between vectors A and B is:\n\n$$\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\cdot \\|\\mathbf{B}\\|}$$\n\n, where A⋅B is the dot product of the vectors, and |A|⋅|B| is the product of each vector's magnitude.\nThe result ranges from -1 to 1, where:\n\n\t-1: Vectors are opposite.\n\t0: Vectors are unrelated.\n\t1: Vectors are identical.\n\n### 5.3.2 Apply Cosine Similarity to Question & Answer Vector Embeddings\nThe cell below creates question and answer pair by collecting all the answer sentences whose componentOrder is between the currentQuestionComponentOrder and the nextQuestionComponentOrder. Then the the consine similarity was applied to the question and answer sentence vector embeddings."
  },
  {
   "cell_type": "code",
   "id": "9226bac4-a0c5-458d-9121-58c8130fbd2b",
   "metadata": {
    "language": "python",
    "name": "py_calc_cosine_sim_QA"
   },
   "outputs": [],
   "source": "# Rename the columns so it's easier to use them when comparing \n# and add a new column that capture the id of the next question\nquestions_vec_rn_df = (questions_vec_df\n                        .rename({\"speakerTypeName\": \"questionSpeakerTypeName\"\n                                 , \"transcriptPersonName\": \"questionTranscriptPersonName\"\n                                 , \"transcriptPersonId\":\"questionTranscriptPersonId\"\n                                 , \"proId\": \"questionProId\"\n                                 , \"transcriptComponentTypeId\": \"questionTranscriptComponentTypeId\"\n                                 , \"transcriptComponentId\": \"questionTranscriptComponentId\"\n                                 , \"componentOrder\": \"currentQuestionOrder\"\n                                 , \"processedText\": \"question\"})\n                        .with_column(\"nextQuestionOrder\"\n                                     ,snow_funcs.lead(\"currentQuestionOrder\")\n                                                    .over(Window.partition_by(\"tradingItemId\", \"transcriptId\")\n                                                                .order_by(\"currentQuestionOrder\"))\n                                    )\n                    )\n# Jon the questions and answers and get the cosine simularity between the questionVec\n# and answerSentenceVec \nquestions_answers_cos_df = (questions_vec_rn_df\n        .join(answers_vec_df, ((questions_vec_rn_df.tradingItemId == answers_vec_df.tradingItemId)\n                    & (questions_vec_rn_df.transcriptId == answers_vec_df.transcriptId)\n                    & (answers_vec_df.componentOrder.between(questions_vec_rn_df.currentQuestionOrder\n                                                                              , snf_ifnull(questions_vec_rn_df.nextQuestionOrder\n                                                                                            , snow_funcs.lit(10000))))\n                    )\n                )\n        .select(questions_vec_rn_df.calldate.as_(\"calldate\") ,questions_vec_rn_df.enteredDate.as_(\"enteredDate\")\n                , questions_vec_rn_df.fiscalyearquarter.as_(\"fiscalyearquarter\")\n                , questions_vec_rn_df.calendarYearQuarter.as_(\"calendarYearQuarter\")\n                , questions_vec_rn_df.tradingItemId.as_(\"tradingItemId\"), questions_vec_rn_df.companyId.as_(\"companyId\")\n                , questions_vec_rn_df.companyName.as_(\"companyName\"), questions_vec_rn_df.headline.as_(\"headline\")\n                , questions_vec_rn_df.transcriptId.as_(\"transcriptId\"), questions_vec_rn_df.questionSpeakerTypeName\n                , questions_vec_rn_df.questionTranscriptPersonName, questions_vec_rn_df.questionTranscriptPersonId\n                , questions_vec_rn_df.questionProId,  answers_vec_df.speakerTypeName.as_(\"answerSpeakerTypeName\")\n                , answers_vec_df.transcriptPersonName.as_(\"answerTranscriptPersonName\")\n                , answers_vec_df.transcriptPersonId.as_(\"answerTranscriptPersonId\")\n                , answers_vec_df.proId.as_(\"answerProId\"), questions_vec_rn_df.questionTranscriptComponentTypeId\n                , answers_vec_df.transcriptComponentTypeId.as_(\"answerTranscriptComponentTypeId\")\n                , questions_vec_rn_df.questionTranscriptComponentId\n                , answers_vec_df.transcriptComponentId.as_(\"answerTranscriptComponentId\")\n                , questions_vec_rn_df.currentQuestionOrder, questions_vec_rn_df.nextQuestionOrder\n                , answers_vec_df.componentOrder.as_(\"answerOrder\")\n                , answers_vec_df.sentenceOrder.as_(\"answerSentenceOrder\")\n                , questions_vec_rn_df.question, answers_vec_df.componentText.as_(\"answer\")\n                , answers_vec_df.processedText.as_(\"answerSentence\")\n                , questions_vec_rn_df.questionSentenceVec, answers_vec_df.answerSentenceVec\n                , snf_vector_cosine_similarity(snow_funcs.col(\"questionSentenceVec\")\n                                               , snow_funcs.col(\"answerSentenceVec\")).as_(\"questionAnswerCosSim\")\n               )\n      .sort(questions_vec_rn_df.nextQuestionOrder.asc_nulls_last()\n            , answers_vec_df.componentOrder.asc_nulls_last()\n            , answers_vec_df.sentenceOrder.asc_nulls_last()\n        )\n     ).cache_result()\n\nquestions_answers_cos_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87af385b-50e0-4574-b7cf-5ededd2124fb",
   "metadata": {
    "name": "md_533_cosine_sim_q_prep_remarks",
    "collapsed": false
   },
   "source": "### 5.3.3 Apply Cosine Similarity to Question & Prepared Remarks Vector Embeddings\nThe cell below pairs all prepared remarks sentences to questions. Then the the consine similarity was applied to the question and prepared remarks sentence vector embeddings."
  },
  {
   "cell_type": "code",
   "id": "93ffc948-073a-475b-a29c-13302d790005",
   "metadata": {
    "language": "python",
    "name": "py_calc_cosine_sim_q_prep_remark"
   },
   "outputs": [],
   "source": "questions_prepared_remarks_cos_df = (questions_vec_rn_df\n                    .join(prepared_remarks_vec_df, ((questions_vec_rn_df.tradingItemId == prepared_remarks_vec_df.tradingItemId)\n                                                & (questions_vec_rn_df.transcriptId == prepared_remarks_vec_df.transcriptId)\n                                            )\n                    )\n                  .select(questions_vec_rn_df.calldate.as_(\"calldate\") ,questions_vec_rn_df.enteredDate.as_(\"enteredDate\")\n                          , questions_vec_rn_df.fiscalyearquarter.as_(\"fiscalyearquarter\")\n                          , questions_vec_rn_df.calendarYearQuarter.as_(\"calendarYearQuarter\")\n                          , questions_vec_rn_df.tradingItemId.as_(\"tradingItemId\"), questions_vec_rn_df.companyId.as_(\"companyId\")\n                          , questions_vec_rn_df.companyName.as_(\"companyName\"), questions_vec_rn_df.headline.as_(\"headline\")\n                          , questions_vec_rn_df.transcriptId.as_(\"transcriptId\"), questions_vec_rn_df.questionSpeakerTypeName\n                          , questions_vec_rn_df.questionTranscriptPersonName, questions_vec_rn_df.questionTranscriptPersonId\n                          , questions_vec_rn_df.questionProId, questions_vec_rn_df.questionTranscriptComponentTypeId\n                          , questions_vec_rn_df.questionTranscriptComponentId, questions_vec_rn_df.currentQuestionOrder\n                          , questions_vec_rn_df.nextQuestionOrder, questions_vec_rn_df.question, questions_vec_rn_df.questionSentenceVec\n                          , prepared_remarks_vec_df.componentOrder.as_(\"executiveRemarkComponentOrder\")\n                          , prepared_remarks_vec_df.sentenceOrder.as_(\"executiveRemarkSentenceOrder\")\n                          , prepared_remarks_vec_df.componentText.as_(\"executiveRemark\")\n                          , prepared_remarks_vec_df.processedText.as_(\"executiveSentence\")\n                          , prepared_remarks_vec_df.PrepRemarkSentenceVec.as_(\"executiveVec\")\n                          ,snf_vector_cosine_similarity(questions_vec_rn_df.questionSentenceVec\n                                                        , prepared_remarks_vec_df.prepRemarkSentenceVec).as_(\"questionExecCosSim\")\n                         )\n                  .sort(questions_vec_rn_df.nextQuestionOrder.asc_nulls_last()\n                        , prepared_remarks_vec_df.componentOrder.asc_nulls_last()\n                        , prepared_remarks_vec_df.sentenceOrder.asc_nulls_last())\n                 ).cache_result()\n\nquestions_prepared_remarks_cos_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50df1066-d89b-42ad-b7b9-ae16a5e4b409",
   "metadata": {
    "name": "md_54_top60_sentences",
    "collapsed": false
   },
   "source": "## 5.4 Top 60% Sentences\nUtilizing the top 60% of prepared remarks identified as generating the most consistent LLM output. For further details on the experiment, please refer to the 'LLM Robustness Check' section in the whitepaper.\n\n### 5.4.2 Concat Top 60% Answer Sentences\nAfter selecting the top 60% most similar answer sentences, we concat the answer sentences on the question level."
  },
  {
   "cell_type": "code",
   "id": "d1002c80-e487-434d-af72-ca2fb0ffbbee",
   "metadata": {
    "language": "python",
    "name": "py_concat_top_60_answers"
   },
   "outputs": [],
   "source": "# First, add two new columns where similarityRank is the row number within \n# each currentQuestionOrder order by the cosin similarity between question sentences \n# and answer sentences, second add a flag column,toKeepFlag, with a 1 \n# if the sentence are part of top 60% \nqa_sim_rank_df = (questions_answers_cos_df\n                       .with_columns([\"similarityRank\"\n                                      , \"answerSentencesCount\"]\n                                    ,[snow_funcs.row_number().over(Window.partition_by(\"tradingItemId\", \"transcriptId\"\n                                                                                       , \"currentQuestionOrder\")\n                                                                        .order_by(snow_funcs.col(\"questionAnswerCosSim\").desc())\n                                                                  )\n                                      ,snow_funcs.count('*').over(Window.partition_by(\"tradingItemId\", \"transcriptId\"\n                                                                                      , \"currentQuestionOrder\")\n                                                                 )\n                                    ])\n                        .with_column(\"toKeepFlag\"\n                                    , snow_funcs.when(snow_funcs.col(\"answerSentencesCount\") == 1\n                                                      , snow_funcs.lit(1))\n                                                .when(snow_funcs.col(\"similarityRank\") <= \n                                                                    snow_funcs.col(\"answerSentencesCount\") * 0.67\n                                                      , snow_funcs.lit(1))\n                                                .otherwise(snow_funcs.lit(0))\n                                    )\n                      )\n\n# Filter out Top 60% sentences, concatinate them and count the tokens for answer and sixtyPercentAnswer\nq_w_sixty_perc_a_df = (qa_sim_rank_df\n                        .filter(qa_sim_rank_df.toKeepFlag == 1)\n                        .group_by(\"callDate\", \"enteredDate\", \"fiscalYearQuarter\", \"calendarYearQuarter\"\n                                  , \"tradingItemId\", \"companyId\", \"companyName\", \"headline\", \"transcriptId\"\n                                  , \"questionSpeakerTypeName\", \"questionTranscriptPersonName\", \"questionTranscriptPersonId\"\n                                  , \"questionProId\", \"answerSpeakerTypeName\", \"answerTranscriptPersonName\"\n                                  , \"answerTranscriptPersonId\", \"answerProId\", \"questionTranscriptComponentTypeId\"\n                                  , \"answerTranscriptComponentTypeId\", \"questionTranscriptComponentId\"\n                                  , \"answerTranscriptComponentId\", \"currentQuestionOrder\", \"nextQuestionOrder\"\n                                  , \"answerOrder\", \"question\", \"answer\")\n                       .agg(snow_funcs.array_to_string(snow_funcs.array_agg(snow_funcs.col(\"answerSentence\"))\n                                                        , snow_funcs.lit(' ')).as_(\"sixtyPercentAnswer\"))\n                       .with_columns([\"answerTokenCount\"\n                                     , \"sixtyPercentAnswerTokenCount\"]\n                                     ,[snf_count_tokens(snow_funcs.lit(completion_model),snow_funcs.col(\"answer\"))\n                                      , snf_count_tokens(snow_funcs.lit(completion_model),snow_funcs.col(\"sixtyPercentAnswer\"))]\n                           )\n                      \n                      ).cache_result()\nq_w_sixty_perc_a_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa16aa95-4ae8-4df0-95a2-d1f9ed43f30f",
   "metadata": {
    "name": "md_543_concat_top60_prep_remarks",
    "collapsed": false
   },
   "source": "### 5.4.3 Concat Top 60% Prepared Remarks Sentences\nSimilarly, after selecting the top 60% most similar prepared remarks sentences, we concat the prepared remarks sentences on the question level."
  },
  {
   "cell_type": "code",
   "id": "887bd0f0-005e-40ce-9abb-c09b0771b121",
   "metadata": {
    "language": "python",
    "name": "py_concat_top60_prep_remarks",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# First, add two new columns where similarityRank is the row number within each currentQuestionOrder \n# order by the cosin similarity between question sentences  and answer sentences, second add a flag column,toKeepFlag, with a 1 \n# if the sentence are part of top 60% \nexec_questions_rank_df = (questions_prepared_remarks_cos_df\n                                .with_columns([\"similarityRank\"\n                                               , \"executiveRemarksSentencesCount\"]\n                                              ,[snow_funcs.row_number().over(Window.partition_by(\"tradingItemId\"\n                                                                                                 , \"transcriptId\"\n                                                                                                 , \"currentQuestionOrder\")\n                                                                                    .order_by(snow_funcs.col(\"questionExecCosSim\").desc_nulls_last()))\n                                                , snow_funcs.count('*').over(Window.partition_by(\"tradingItemId\", \"transcriptId\"\n                                                                                                 , \"currentQuestionOrder\"))])\n                                .with_column(\"toKeepFlag\"\n                                     , snow_funcs.when(snow_funcs.col(\"executiveRemarksSentencesCount\") == 1\n                                                       , snow_funcs.lit(1))\n                                        .when(snow_funcs.col(\"similarityRank\") <= snow_funcs.col(\"executiveRemarksSentencesCount\") * 0.67\n                                                        , snow_funcs.lit(1))\n                                        .otherwise(snow_funcs.lit(0)))\n                                 )\n\n# Filter out Top 60% sentences and concatinate them and recreate the full opening remarks\nexec_questions_sixty_perc_df = (exec_questions_rank_df\n                                .filter(exec_questions_rank_df.toKeepFlag == 1)\n                                .group_by(\"callDate\", \"enteredDate\", \"fiscalYearQuarter\", \"calendarYearQuarter\"\n                                          , \"tradingItemId\", \"companyId\", \"companyName\", \"headline\", \"transcriptId\"\n                                          , \"questionSpeakerTypeName\", \"questionTranscriptPersonName\"\n                                          , \"questionTranscriptPersonId\", \"questionProId\"\n                                          , \"questionTranscriptComponentTypeId\", \"questionTranscriptComponentId\"\n                                          , \"currentQuestionOrder\", \"nextQuestionOrder\", \"executiveRemarkComponentOrder\"\n                                          , \"question\")\n                                        .agg(snow_funcs.listagg(snow_funcs.col(\"executiveSentence\"), ' ').as_('sixtyPercentExecutiveRemark')\n                                            ,snow_funcs.listagg(snow_funcs.col(\"executiveRemark\"), '\\n\\n').as_('fullExecutiveRemark'))\n                                   ).cache_result()\n\nexec_questions_sixty_perc_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ddb0cf0-76c9-49f2-bab2-41de606b73ce",
   "metadata": {
    "name": "md_6_llm_ready_data",
    "collapsed": false
   },
   "source": "# 6. Working with the Data: LLM Ready Data\n\nUsing a LLM to answer analysts questions based only on the prepared remarks and the previous questions and answers will give an indication if executives are proactive.\n\n## 6.1 Using Snowflake Cortex AI\nWhen calling the Snowflake Cortex COMPLETE function, messages are organized into distinct roles—system, user, and assistant—to structure and guide interactions. Each role serves a specific purpose:\n\nSystem: Provides instructions that define the context or behavior of the model. It's like setting the rules or tone for the conversation. Example: \"You are a helpful assistant that answers questions about technology in a concise manner.\"\n\nUser: Represents the input or queries made by the person interacting with the model. These are the prompts or requests that the model responds to. Example: \"What is the purpose of the OpenAI API?\"\n\nAssistant: Reflects the model's response to the user's query, shaped by the system's instructions and the user's input. Example: \"The OpenAI API is designed to enable developers to integrate language models into their applications for tasks like answering questions, generating content, and more.\"\n\nIn our research, executive prepared remarks are labelled as assistant messages, analyst's questions as User messages and executive answers as Assistant messages\n\n### 6.1.1 Construct COMPLETE Assistant Message with QA Pair Snippet\n\nCreate a object that has the Analyst question with the role as user and the answer with the role as assitsant example:\n```\n[\n  {\n    \"content\": \"Analyst Question\",\n    \"role\": \"user\"\n  },\n  {\n    \"content\": \"Executive 60% Answer\",\n    \"role\": \"assistant\"\n  }\n]\n```"
  },
  {
   "cell_type": "code",
   "id": "83172bf3-90ef-469e-b758-9b91837771a4",
   "metadata": {
    "language": "python",
    "name": "py_construct_QA_pair_snippet"
   },
   "outputs": [],
   "source": "\nq_w_sixty_perc_a_prompt_df = (q_w_sixty_perc_a_df\n                                     .with_columns([\"questionPromptSnippet\", \"answerPromptSnippet\"]\n                                                  ,[snow_funcs.object_construct(\n                                                            snow_funcs.lit('role'), snow_funcs.lit('user')\n                                                            , snow_funcs.lit('content')\n                                                            , snow_funcs.replace(q_w_sixty_perc_a_df.question\n                                                                                 , snow_funcs.lit('\\r'), snow_funcs.lit(''))\n                                                        )\n                                                        , snow_funcs.object_construct(\n                                                            snow_funcs.lit('role'), snow_funcs.lit('assistant')\n                                                            , snow_funcs.lit('content')\n                                                            , snow_funcs.replace(q_w_sixty_perc_a_df.sixtyPercentAnswer\n                                                                                , snow_funcs.lit('\\r'), snow_funcs.lit(''))\n                                                        )])\n                             ).cache_result()\n#transComp_pppQPairTop60AnswerConcat_df\nq_w_sixty_perc_a_prompt_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e3b400f-8459-48d4-980f-ab4b99582361",
   "metadata": {
    "name": "md_612_construct_complete_prompt",
    "collapsed": false
   },
   "source": "### 6.1.2 Construct Snowflake Cortex COMPLETE Assistant Message with Prepared Remarks Snippets\n\nCreate a object that has the Analyst question with the role as user and the 60% prepared remarks with the role as assitsant example:\n```\n[\n  {\n    \"content\": \"Analyst Question\",\n    \"role\": \"user\"\n  },\n  {\n    \"content\": \"60% Prepared Remarks\",\n    \"role\": \"assistant\"\n  }\n]\n```"
  },
  {
   "cell_type": "code",
   "id": "e803f650-f6ea-4d80-9e49-a55a70b3029a",
   "metadata": {
    "language": "python",
    "name": "py_generate_assistan_messages"
   },
   "outputs": [],
   "source": "exec_questions_sixty_perc_prompt_df = (exec_questions_sixty_perc_df\n                                     .with_columns([\"questionPromptSnippet\"\n                                                   , \"remarksPromptSnippet\"]\n                                                  , [snow_funcs.object_construct(\n                                                            snow_funcs.lit('role'), snow_funcs.lit('user')\n                                                            , snow_funcs.lit('content')\n                                                            , snow_funcs.replace(exec_questions_sixty_perc_df.question\n                                                                                 , snow_funcs.lit('\\r'), snow_funcs.lit(''))\n                                                        )\n                                                     , snow_funcs.object_construct(\n                                                            snow_funcs.lit('role'), snow_funcs.lit('assistant')\n                                                            , snow_funcs.lit('content')\n                                                            , snow_funcs.replace(exec_questions_sixty_perc_df.sixtyPercentExecutiveRemark\n                                                                                , snow_funcs.lit('\\r'), snow_funcs.lit(''))\n                                                        )\n                                                    ])\n                                            ).cache_result()\nexec_questions_sixty_perc_prompt_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8652c71-32ee-4f1e-9972-24f0a47cb5b7",
   "metadata": {
    "name": "md_collect_complete_messages",
    "collapsed": false
   },
   "source": "## 6.2 Collect All Messages for Cortex COMPLETE\nAll question pairs with prepare remarks and answers come together to form an LLM prompt following the iterative process such that:\n\n1. 'user': 'From the perspective of a top executive, please answer the following question raised by a financial analyst during an earnings conference call. Knowledge cutoff date: '  \n2. 'assistant': 60% prepared remarks  \n3. 'user': question 1  \n4. 'assistant': 60% answer 1  \n5. ...  \n6. ...  \n7. 'user': question n  \n\n### 6.2.1 LLM Ready Prompt Messages\nIn the dataframe below, the prompt column has all the messages in 1 list. This is the prompt for the LLM."
  },
  {
   "cell_type": "code",
   "id": "16c79a7e-eb31-4731-acce-da414dfeecd4",
   "metadata": {
    "language": "python",
    "name": "py_construct_full_prompt"
   },
   "outputs": [],
   "source": "#transComp_pppQPairTop60AnswerConcat_df -> q_w_sixty_perc_a_prompt_df\n\nq_w_sixty_perc_a_LLM_promp_df = (q_w_sixty_perc_a_prompt_df\n                                .with_columns([\"initPrompt\", \"concatenatedPredecessors\"]\n                                             ,[snow_funcs.array_construct(\n                                                    snow_funcs.object_construct(snow_funcs.lit('role'), snow_funcs.lit('user')\n                                                                               , snow_funcs.lit('content')\n                                                                               , snow_funcs.concat(snow_funcs.lit('From the perspective of a top executive, please answer the following question raised by a financial analyst during an earnings conference call. Knowledge cutoff date: ')\n                                                                                                  , snow_funcs.to_char(exec_questions_sixty_perc_prompt_df.callDate))))\n                                                 \n                                             , snow_funcs.array_flatten(\n                                                 snow_funcs.array_agg(snow_funcs.array_construct(q_w_sixty_perc_a_prompt_df.questionPromptSnippet\n                                                                                                 , q_w_sixty_perc_a_prompt_df.answerPromptSnippet))\n                                                                .over(Window.partition_by(q_w_sixty_perc_a_prompt_df.transcriptid)\n                                                                            .order_by(q_w_sixty_perc_a_prompt_df.CURRENTQUESTIONORDER)\n                                                                            .rows_between(Window.unboundedPreceding, -1))\n                                                        \n                                                )]\n                                )\n                                .with_column(\"prompt\"\n                                            , snow_funcs.array_cat(snow_funcs.col(\"initPrompt\")\n                                                                  ,snow_funcs.array_cat(snow_funcs.col(\"concatenatedPredecessors\")\n                                                                                       , snow_funcs.array_construct(snow_funcs.col(\"questionPromptSnippet\")))\n                                                                  ))\n                            ).cache_result()\nq_w_sixty_perc_a_LLM_promp_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c509c9a6-2350-4293-9acc-1fb5c540830e",
   "metadata": {
    "name": "md_63_collect_COMPLETE_responses",
    "collapsed": false
   },
   "source": "## 6.3 Collect LLM Response\n\n### 6.3.1 Apply LLM Completion\nWe apply the SNOWFLAKE.CORTEX.COMPLETE function on the prompt column using the model defined by `completion_model`and collect the LLM response."
  },
  {
   "cell_type": "code",
   "id": "cf47c386-0f27-4db2-b7cf-d096c376e305",
   "metadata": {
    "language": "python",
    "name": "py_call_complete"
   },
   "outputs": [],
   "source": "opts = CompleteOptions(temperature = 0)\n\nquestion_LLM_answer_raw_df = (q_w_sixty_perc_a_LLM_promp_df\n                                    .with_column(\"LLMAnswer\"\n                                                , complete(model=completion_model\n                                                           , prompt=q_w_sixty_perc_a_LLM_promp_df.prompt\n                                                          , options=opts))\n                               ).cache_result()\n\nquestion_LLM_answer_raw_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0035b140-240c-45bd-8d0a-e51764d0a3eb",
   "metadata": {
    "name": "md_632_clean_up_LLM_responses",
    "collapsed": false
   },
   "source": "### 6.3.2 Clean Up LLM Response\n\nExtract only the actual message from the LLM from the responses"
  },
  {
   "cell_type": "code",
   "id": "9a0942e7-4ad8-410d-9bd3-b55fe83b8285",
   "metadata": {
    "language": "python",
    "name": "py_extract_message"
   },
   "outputs": [],
   "source": "question_LLM_answer_df = (question_LLM_answer_raw_df\n                                        .with_column(\"cleanLLMAnswer\"\n                                         , snow_funcs.to_varchar(question_LLM_answer_raw_df.LLMANSWER['choices'][0]['messages']))\n                                      .select(\"callDate\", \"tradingItemId\", \"transcriptId\", \"headline\"\n                                              , \"questionTranscriptPersonName\", \"questionTranscriptPersonId\"\n                                              , \"questionProId\", \"answerTranscriptPersonName\", \"answerTranscriptPersonId\"\n                                              , \"answerProId\",\"questionTranscriptComponentId\", \"answerTranscriptComponentId\"\n                                              , \"question\", \"answer\", \"cleanLLMAnswer\")\n                                     ).cache_result()\n\nquestion_LLM_answer_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb29bb53-5a68-4d00-978a-6302b1eb16a8",
   "metadata": {
    "name": "md_64_summarize_text",
    "collapsed": false
   },
   "source": "## 6.4 Summarize Text\n\nUse the Snowflake Cortex Summarize function to summarize the question, answer and LLM answer. This is done so we can compare them later. "
  },
  {
   "cell_type": "code",
   "id": "1f6f2c26-6553-4bd0-915f-f838cf3779f5",
   "metadata": {
    "language": "python",
    "name": "py_summarize",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "question_LLM_answer_summarize_df = (question_LLM_answer_df\n                                            .with_columns([\"summarizeQuestion\", \"summarizeAnswer\", \"summarizeCleanLLMAnswer\"]\n                                                         ,[summarize(question_LLM_answer_df.question)\n                                                          ,summarize(question_LLM_answer_df.answer)\n                                                           ,summarize(question_LLM_answer_df.cleanLLMAnswer)\n                                                          ]\n                                                    )\n                                         ).cache_result()\n\nquestion_LLM_answer_summarize_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cde8c8fb-317d-40ec-b17b-a7efc118fc78",
   "metadata": {
    "name": "md_7_factor_construction",
    "collapsed": false
   },
   "source": "# 7. Working with the Data: Factor Construction\n## 7.1 Executive On/Off Topic Factor\nWhen an executive answer is semantically similar (dissimilar) to the analyst’s question, it suggests that the answer uses language and concepts similar to (different from) the analyst question, indicating it is on-topic (off-topic). To determine semantic closeness, we vector-embed the question-and-answer texts and calculate a cosine similarity score between the two vectors.\n\n### 7.1.1 Question vs Executive Answer Cosine Similarity"
  },
  {
   "cell_type": "code",
   "id": "393902fc-3b03-419a-9c8c-11afbf080d5a",
   "metadata": {
    "language": "python",
    "name": "py_q_v_a_cosine_sim"
   },
   "outputs": [],
   "source": "qa_exec_on_off_topic_factor_df = (question_LLM_answer_summarize_df\n                                        .with_columns([\"questionVec\", \"answerVec\"]\n                                                    ,[embed_text_768(embedding_model, question_LLM_answer_summarize_df.summarizeQuestion)\n                                                     ,embed_text_768(embedding_model, question_LLM_answer_summarize_df.summarizeAnswer)])\n                                        .with_column(\"execOnOffTopicFactor\"\n                                                    , snf_vector_cosine_similarity(snow_funcs.col(\"questionVec\"), snow_funcs.col(\"answerVec\")))\n                                    ).cache_result()\nqa_exec_on_off_topic_factor_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38be14f8-7a72-4aab-984c-43aa3e76ff97",
   "metadata": {
    "name": "md_712_mean_on_off_topic",
    "collapsed": false
   },
   "source": "### 7.1.2 Transcript Mean Executive On/Off Topic Factor\nCosine similarity scores are averaged at the transcript level. A high (low) Cosine Similarity Score indicates an On (Off) Topic Executive.\n\n"
  },
  {
   "cell_type": "code",
   "id": "9dc6d770-91aa-484e-8ab5-a33f8968ce46",
   "metadata": {
    "language": "python",
    "name": "py_mean_on_off_topic"
   },
   "outputs": [],
   "source": "qa_exec_on_off_topic_factor_df.select(snow_funcs.avg(\"execOnOffTopicFactor\").as_(\"transcriptLevelExecOnOffTopicFactor\"))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "916210b3-8d62-4bac-9a1a-13fbc0d08ffc",
   "metadata": {
    "name": "md_72_executive_pro_re_factor",
    "collapsed": false
   },
   "source": "## 7.2 Executive Proactive/Reactive Factor\n### 7.2.1 Question vs LLM Answer Cosine Similarity\nSince the LLM answers only within the context of information provided in the prepared remarks, a high (low) cosine similarity score indicates that the LLM answers are semantically similar (dissimilar) the questions, reflecting the executives are proactive (reactive)."
  },
  {
   "cell_type": "code",
   "id": "d708d6b1-609a-42ab-a0e0-94b858de0d4f",
   "metadata": {
    "language": "python",
    "name": "py_q_llmA_cosine_sim"
   },
   "outputs": [],
   "source": "q_exec_proactive_reactive_factor_df = (question_LLM_answer_summarize_df\n                                        .with_columns([\"questionVec\", \"LLMAnswerVec\"]\n                                                    ,[embed_text_768(embedding_model, question_LLM_answer_summarize_df.summarizeQuestion)\n                                                    ,embed_text_768(embedding_model, question_LLM_answer_summarize_df.summarizeCleanLLMAnswer)])\n                                        .with_column(\"execProactiveReactiveFactor\"\n                                                    , snf_vector_cosine_similarity(snow_funcs.col(\"questionVec\"), snow_funcs.col(\"LLMAnswerVec\")))\n                                    ).cache_result()\nq_exec_proactive_reactive_factor_df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5999994a-81e2-428f-ab24-4a5eeb72e150",
   "metadata": {
    "name": "md_722_mean_pro_re_factor",
    "collapsed": false
   },
   "source": "### 7.2.2 Transcript Mean Executive Proactive/Reactive Factor\nSimilar to the construction of the Executive On/Off Topic factor, both the LLM answers and questions are summarized, vector-embedded and cosine similarity scores are averaged at the transcript level."
  },
  {
   "cell_type": "code",
   "id": "60000592-a4de-4d5c-9cde-a6217108f124",
   "metadata": {
    "language": "python",
    "name": "py_mean_pro_re_factor"
   },
   "outputs": [],
   "source": "q_exec_proactive_reactive_factor_df.select(snow_funcs.avg(\"execProactiveReactiveFactor\").as_(\"transcriptLevelexecProactiveReactiveFactor\"))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98027875-ffd6-40a2-b098-3fa6e41ef857",
   "metadata": {
    "name": "md_8_ask_your_own_questions",
    "collapsed": false
   },
   "source": "# 8. Ask your own questions\n\nBased on on the prepared remarks, the questions and answers during the call you can use Cortex Complete with one of the included LLMs to ask your own questions. The code below will use the existing Questions and Answers and then append your question to it and use it with the choosen LLM."
  },
  {
   "cell_type": "code",
   "id": "d54d9a4a-a621-4101-b80f-8821a9008fde",
   "metadata": {
    "language": "python",
    "name": "py_ask_your_own_questions"
   },
   "outputs": [],
   "source": "message_history = json.loads(question_LLM_answer_raw_df\n                             .sort(question_LLM_answer_raw_df.questionTranscriptComponentId.desc_nulls_last()\n                                   , question_LLM_answer_raw_df.answerTranscriptComponentId.desc_nulls_last())\n                             .limit(1)\n                             .select(\"prompt\", \"sixtyPercentAnswer\")\n                             .with_column(\"prompt_new\"\n                                          , snow_funcs.array_append(snow_funcs.col(\"prompt\")\n                                                                    ,snow_funcs.object_construct(snow_funcs.lit('role'), snow_funcs.lit('assistant')\n                                                                                                 , snow_funcs.lit('content')\n                                                                                                 , snow_funcs.replace(question_LLM_answer_raw_df.sixtyPercentAnswer\n                                                                                                                      , snow_funcs.lit('\\r'), snow_funcs.lit('')))\n                                                                   )\n                            )\n                             .collect()[0]['PROMPT_NEW'])\n\nst.write(f\"Using model: {completion_model}\")\n\n#Generate a response\nuser_input_question = st.text_input(\"Ask me a question\")\n\nask= st.button(\"Ask\", key = \"button_ask\")\nif ask: \n    message_history.append({'role':'user', 'content': user_input_question})\n    data = complete(model=completion_model, prompt=message_history, session=session)\n    with st.chat_message(\"model\", avatar =\"assistant\"):\n        st.write(data)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3c21-f09e-4994-aa9a-1db01028f0d3",
   "metadata": {
    "name": "md_9_result_summary",
    "collapsed": false
   },
   "source": "# 9. Results & Summary\nThis research underscores the significant impact of executive communication styles during earnings calls on firm performance. Proactive executives who anticipate market concerns and provide concise, on-topic responses foster transparency, aligning with investor expectations and driving superior returns. The findings demonstrate that firms with Efficient Communicators achieve statistically significant outperformance, while Total Redirectors suffer from diminished confidence and underperformance. These insights validate the critical role of strategic communication in shaping investor perceptions and influencing market outcomes.\n\nAdvanced analytical tools, such as vector embeddings and cosine similarity metrics, enable nuanced evaluations of executive-analyst interactions, revealing measurable performance effects across different communication styles. While large language models (LLMs) enhance feature extraction, challenges like forward-looking bias and inconsistency highlight the need for caution in time-sensitive tasks. Overall, the integration of proactive, clear, and relevant communication strategies remains paramount in fostering investor trust and maximizing financial success in a competitive marketplace."
  }
 ]
}